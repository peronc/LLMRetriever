{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Large Language Model Retriever\n",
    "##Introduction\n",
    "In this notebook, I will demonstrate how a Large Language Model (LLM) based retriever can work.\n",
    "\n",
    "The primary concept involves having a list of documents (__chunks of text__) stored in a database, which could be a text file, a relational database, or any other type of storage system.\n",
    "\n",
    "The retriever can retrieve documents in their entirety or based on specific criteria and filters.\n",
    "\n",
    "These documents are then passed to the LLM, which evaluates their relevance and determines whether they can effectively answer a user's question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Import Library\n",
    "This notebook leverages LangChain and the OpenAI model deployed on Azure.\n",
    "\n",
    "First, we import the necessary standard libraries, including os, langchain, and dotenv.\n",
    "\n",
    "Next, we import my llm_retrieve class, which provides several static methods essential for performing the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from LLMRetrieverLib.retriever import llm_retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Setting variables\n",
    "Following that, we need to import the necessary variables required for utilizing Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "azure_deployment = os.getenv(\"AZURE_DEPLOYMENT\")\n",
    "temperature = float(os.getenv(\"TEMPERATURE\"))\n",
    "api_key  = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"API_VERSION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Define database\n",
    "In a real use case, I retrieve the chunks from a relational database or an Azure Search database, consisting of 95 text segments semantically split from two Microsoft Word documents, totaling 33 pages.\n",
    "\n",
    "To simplify this example, we will define the database to analyze as a list of text segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Chunk 1: This document contains information about topic A.\",\n",
    "    \"Chunk 2: Insights related to topic B can be found here.\",\n",
    "    \"Chunk 3: This chunk discusses topic C in detail.\",\n",
    "    \"Chunk 4: Further insights on topic D are covered here.\",\n",
    "    \"Chunk 5: Another chunk with more data on topic E.\",\n",
    "    \"Chunk 6: Extensive research on topic F is presented.\",\n",
    "    \"Chunk 7: Information on topic G is explained here.\",\n",
    "    \"Chunk 8: This document expands on topic H. It also talk about topic B\",\n",
    "    \"Chunk 9: More insights about topic I are given.\",\n",
    "    \"Chunk 10: Finally, a discussion of topic J. This document doesn't contain information about topic B.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##User question\n",
    "The user wants to learn more about a specific topic in the database, so they ask a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I would like to know something about topic B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Initiate LLM\n",
    "Now I create a LLM to perform the analisys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = AzureChatOpenAI(api_key=api_key, azure_endpoint=endpoint, azure_deployment=azure_deployment, api_version=api_version,temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Final part\n",
    "First, I retrieve the relevant chunks using the LLM to determine which ones can be used to provide an answer.\n",
    "\n",
    "I utilize multi-threading to simultaneously send multiple requests to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMRetrieverLib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m relevant_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mLLMRetrieverLib\u001b[49m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39mllm_retrieve\u001b[38;5;241m.\u001b[39mprocess_chunks_in_parallel(llm, question, documents, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LLMRetrieverLib' is not defined"
     ]
    }
   ],
   "source": [
    "relevant_chunks = LLMRetrieverLib.retriever.llm_retrieve.process_chunks_in_parallel(llm, question, documents, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have the relevant chunks, I use them to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if relevant_chunks:\n",
    "    final_answer = LLMRetrieverLib.retriever.llm_retrieve.generate_final_answer_with_llm(llm, relevant_chunks, question)\n",
    "    print(\"Final Response:\")\n",
    "    print(final_answer)\n",
    "else:\n",
    "    print(\"No relevant chunks found for the question.\", question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
